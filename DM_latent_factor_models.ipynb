{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pO2SzeNxuZZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"/content/data\"):\n",
        "\n",
        "  # Mount your Google Drive.\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "\n",
        "  # kaggle_creds_path = \"/content/drive/Colab Notebooks\"\n",
        "\n",
        "  ! pip install kaggle --quiet\n",
        "\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! cp \"/content/drive/MyDrive/Colab Notebooks/kaggle.json\" ~/.kaggle/\n",
        "  ! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  ! kaggle datasets download -d hernan4444/anime-recommendation-database-2020\n",
        "  ! mkdir data\n",
        "  ! unzip anime-recommendation-database-2020.zip -d data\n",
        "\n",
        "  # Unmount your Google Drive\n",
        "  # drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2agQLdF0FZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "v5gohhRRvsM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/rating_complete.csv')"
      ],
      "metadata": {
        "id": "GtgUwYeNNg8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counts = df['user_id'].value_counts()\n",
        "# (counts > 100).sum()\n",
        "# (df['anime_id'].value_counts() > 100000).sum()\n",
        "# df['anime_id'].nunique()"
      ],
      "metadata": {
        "id": "Q4uIVKSKNhE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "  user_min_ratings = 500\n",
        "  anime_min_ratings = 10000\n",
        "\n",
        "  # Filter out users with less than user_min_ratings ratings\n",
        "  user_counts = df['user_id'].value_counts()\n",
        "  filtered_users = user_counts[user_counts >= user_min_ratings].index\n",
        "  df = df[df['user_id'].isin(filtered_users)]\n",
        "\n",
        "  # Filter out animes with less than anime_min_ratings ratings\n",
        "  anime_counts = df['anime_id'].value_counts()\n",
        "  filtered_animes = anime_counts[anime_counts >= anime_min_ratings].index\n",
        "  df = df[df['anime_id'].isin(filtered_animes)]\n",
        "\n",
        "# Remap user_id and anime_id\n",
        "user_id_mapping = {id: i for i, id in enumerate(df['user_id'].unique())}\n",
        "anime_id_mapping = {id: i for i, id in enumerate(df['anime_id'].unique())}\n",
        "df['user_id'] = df['user_id'].map(user_id_mapping)\n",
        "df['anime_id'] = df['anime_id'].map(anime_id_mapping)\n",
        "\n",
        "# Rating matrix TOO BIG FOR UNFILTERED DATASET\n",
        "# rating_matrix = df.pivot(index='user_id', columns='anime_id', values='rating') #.fillna(0)\n",
        "# x = rating_matrix.to_numpy()"
      ],
      "metadata": {
        "id": "U1rglPFLTHRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "n = df.shape[0]\n",
        "n_users, n_animes = df['user_id'].nunique(), df['anime_id'].nunique()\n",
        "\n",
        "np.random.seed(42)\n",
        "split = np.random.permutation(n)\n",
        "\n",
        "xy = torch.from_numpy(df.to_numpy()).to(device)  # columns ['user_id', 'anime_id', 'rating']\n",
        "x, y = xy[:, :2], xy[:, 2].float()\n",
        "\n",
        "y = y * 0.1  # RESCALE to [0, 1]\n",
        "\n",
        "def train_val_test_split(t):\n",
        "  t_train = t[split[:int(n*0.8)]]\n",
        "  t_val = t[split[int(n*0.8):int(n*0.9)]]\n",
        "  t_test = t[split[int(n*0.9):]]\n",
        "  return t_train, t_val, t_test\n",
        "\n",
        "x_train, x_val, x_test = train_val_test_split(x)\n",
        "y_train, y_val, y_test = train_val_test_split(y)"
      ],
      "metadata": {
        "id": "qs8gH9VVkCU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# from https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014\n",
        "class FastTensorDataLoader:\n",
        "    \"\"\"\n",
        "    A DataLoader-like object for a set of tensors that can be much faster than\n",
        "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
        "    the dataset and calls cat (slow).\n",
        "    \"\"\"\n",
        "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
        "        \"\"\"\n",
        "        Initialize a FastTensorDataLoader.\n",
        "\n",
        "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
        "        :param batch_size: batch size to load.\n",
        "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
        "            iterator is created out of this object.\n",
        "\n",
        "        :returns: A FastTensorDataLoader.\n",
        "        \"\"\"\n",
        "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
        "        self.tensors = tensors\n",
        "\n",
        "        self.dataset_len = self.tensors[0].shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Calculate # batches\n",
        "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
        "        if remainder > 0:\n",
        "            n_batches += 1\n",
        "        self.n_batches = n_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            self.indices = torch.randperm(self.dataset_len)\n",
        "        else:\n",
        "            self.indices = None\n",
        "        self.i = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.i >= self.dataset_len:\n",
        "            raise StopIteration\n",
        "        if self.indices is not None:\n",
        "            indices = self.indices[self.i:self.i+self.batch_size]\n",
        "            batch = tuple(torch.index_select(t, 0, indices) for t in self.tensors)\n",
        "        else:\n",
        "            batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
        "        self.i += self.batch_size\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n",
        "\n",
        "batch_size = 1_000_000\n",
        "train_dataloader = FastTensorDataLoader(x_train, y_train, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader = FastTensorDataLoader(x_val, y_val, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = FastTensorDataLoader(x_test, y_test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "02FSklP2KR1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline: average rating\n",
        "mse = nn.functional.mse_loss(torch.ones_like(y_test) * y_train.mean(), y_test)\n",
        "print(f\"Test MSE for using average rating as prediction: {mse}\")"
      ],
      "metadata": {
        "id": "sFBEnUp915VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveLatentFactorModel(nn.Module):\n",
        "    def __init__(self, n_users, n_animes, embedding_dim):\n",
        "        super(NaiveLatentFactorModel, self).__init__()\n",
        "        self.user_emb = nn.Embedding(num_embeddings=n_users, embedding_dim=embedding_dim)\n",
        "        self.anime_emb = nn.Embedding(num_embeddings=n_animes, embedding_dim=embedding_dim)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Compute the dot product between user and anime embeddings\n",
        "        x_matrix = self.user_emb.weight @ self.anime_emb.weight.T\n",
        "\n",
        "        # Return the selected indices\n",
        "        return x_matrix[idx[:, 0], idx[:, 1]]"
      ],
      "metadata": {
        "id": "81sSQtjYwAb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentFactorModel(nn.Module):\n",
        "    def __init__(self, n_users, n_animes, embedding_dim):\n",
        "        super(LatentFactorModel, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.user_emb = nn.Embedding(num_embeddings=n_users, embedding_dim=embedding_dim)\n",
        "        self.anime_emb = nn.Embedding(num_embeddings=n_animes, embedding_dim=embedding_dim)\n",
        "\n",
        "        self.user_bias = nn.Parameter(torch.zeros(n_users))\n",
        "        self.anime_bias = nn.Parameter(torch.zeros(n_animes))\n",
        "\n",
        "        self.dot_scale = nn.Parameter(torch.ones(1) * (1/embedding_dim))\n",
        "        self.final_bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, idx):\n",
        "        if isinstance(idx, np.ndarray):\n",
        "          idx = torch.from_numpy(idx)\n",
        "\n",
        "        user_vectors = self.user_emb(idx[:, 0])\n",
        "        anime_vectors = self.anime_emb(idx[:, 1])\n",
        "\n",
        "        out = (user_vectors * anime_vectors).sum(dim=1) / self.embedding_dim  # * self.dot_scale  #\n",
        "\n",
        "        out = out + self.user_bias[idx[:, 0]] + self.anime_bias[idx[:, 1]]\n",
        "        #out = self.user_bias[idx[:, 0]] + self.anime_bias[idx[:, 1]]\n",
        "\n",
        "        out = out + self.final_bias\n",
        "        #out = torch.ones_like(self.user_bias[idx[:, 0]]) * self.final_bias\n",
        "\n",
        "        return torch.sigmoid(out) # + self.bias"
      ],
      "metadata": {
        "id": "kE5oDRdB30t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def training_loop(model, num_epochs):\n",
        "  #optimizer = torch.optim.AdamW(model.parameters(), lr=0.3, weight_decay=0.1)\n",
        "  optimizer = torch.optim.AdamW([\n",
        "    {'params': model.user_emb.parameters(), 'weight_decay': 0.1},\n",
        "    {'params': model.anime_emb.parameters(), 'weight_decay': 0.1},\n",
        "    {'params': [model.user_bias, model.anime_bias], 'weight_decay': 0.0},\n",
        "    {'params': [model.dot_scale, model.final_bias], 'weight_decay': 0}  # min lr = 0.03, max = 0.5\n",
        "  ], lr=0.03) # min lr 0.003\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "    for x_batch, y_batch in tqdm(train_dataloader, desc=\"Steps\", leave=False):\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model(x_batch)\n",
        "      loss = nn.functional.mse_loss(y_pred, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_losses = []\n",
        "      for x_batch, y_batch in val_dataloader:\n",
        "        y_pred = model(x_batch)\n",
        "        val_losses.append(nn.functional.mse_loss(y_pred, y_batch))\n",
        "      val_loss = torch.tensor(val_losses).mean()\n",
        "    if epoch % 5 == 0:\n",
        "      print(f\"train loss: {loss.item()}, val loss: {val_loss.item()}, val rmse: {torch.sqrt(val_loss).item()}\")\n",
        "  print(f\"train loss: {loss.item()}, val loss: {val_loss.item()}, val rmse: {torch.sqrt(val_loss).item()}\")"
      ],
      "metadata": {
        "id": "QL9EpIOlwAfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def training_loop(model, num_epochs):\n",
        "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.3, weight_decay=0.1)\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.user_emb.parameters(), 'weight_decay': 0.1},\n",
        "        {'params': model.anime_emb.parameters(), 'weight_decay': 0.1},\n",
        "        {'params': [model.user_bias, model.anime_bias], 'weight_decay': 0.0},\n",
        "        {'params': [model.dot_scale, model.final_bias], 'weight_decay': 0}  # min lr = 0.03, max = 0.5\n",
        "    ], lr=0.03) # min lr 0.003\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "        epoch_losses = []\n",
        "        for x_batch, y_batch in tqdm(train_dataloader, desc=\"Steps\", leave=False):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x_batch)\n",
        "            loss = nn.functional.mse_loss(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        # calculate mean training loss for the epoch\n",
        "        train_losses.append(sum(epoch_losses) / len(epoch_losses))\n",
        "\n",
        "        # Validation\n",
        "        with torch.no_grad():\n",
        "            epoch_val_losses = []\n",
        "            for x_batch, y_batch in val_dataloader:\n",
        "                y_pred = model(x_batch)\n",
        "                val_loss = nn.functional.mse_loss(y_pred, y_batch)\n",
        "                epoch_val_losses.append(val_loss.item())\n",
        "            # mean validation loss for the epoch\n",
        "            val_losses.append(sum(epoch_val_losses) / len(epoch_val_losses))\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "            print(f\"Epoch {epoch}: train loss: {train_losses[-1]}, val loss: {val_losses[-1]}\")\n",
        "\n",
        "    # plot the training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training loss')\n",
        "    plt.plot(val_losses, label='Validation loss')\n",
        "    plt.title('Training and Validation Losses Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F5YKZRc95cCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LatentFactorModel(n_users, n_animes, embedding_dim=32).to(device)"
      ],
      "metadata": {
        "id": "rnm4fFO6wAiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(model, 25)"
      ],
      "metadata": {
        "id": "JHYpVdOtzMGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egvqgnmXFChP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vw-p02JbFDTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwPyEWhBFCjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnsW_OfoFCou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime_emb = model.anime_emb.weight.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "ylRk8mnJ84oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdlG9JWmw9oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/tmp'"
      ],
      "metadata": {
        "id": "UYQpNeSYATcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f'{base_path}/model_full_emb32.pth')"
      ],
      "metadata": {
        "id": "jzEAmbmx84rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(f'{base_path}/model_full_emb32.pth')\n",
        "anime_emb_matrix = state_dict['anime_emb.weight'].cpu().numpy()\n",
        "np.save(f'{base_path}/anime_emb32.npy', anime_emb_matrix)"
      ],
      "metadata": {
        "id": "Help98_184vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "anime_emb = np.load(f'{base_path}/anime_emb32.npy')"
      ],
      "metadata": {
        "id": "OmczEyBS840J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backup_ae = anime_emb\n",
        "anime_emb.shape"
      ],
      "metadata": {
        "id": "9tmhqwcB8466"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# anime_emb = np.random.rand(backup_ae.shape[0], backup_ae.shape[1]) -0.5  # for sanity check using random values"
      ],
      "metadata": {
        "id": "0dS6ao_40lQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime_emb.shape"
      ],
      "metadata": {
        "id": "3ZPe1mD20lTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime_df = pd.read_csv('data/anime.csv')"
      ],
      "metadata": {
        "id": "SL4ephpfS17D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime_df['anime_id'] = anime_df['MAL_ID'].map(anime_id_mapping)\n",
        "anime_df = anime_df.dropna(subset=['anime_id'])\n",
        "anime_df = anime_df.sort_values(['anime_id']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "XIDkl_DLT4nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anime_df['Genres'].str.contains('Action').to_numpy().sum()"
      ],
      "metadata": {
        "id": "8WPcZ7g8S2Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genre_mask(name):\n",
        "    return anime_df['Genres'].str.contains(name).to_numpy()"
      ],
      "metadata": {
        "id": "JrY2GXpnTcts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_emb = anime_emb[genre_mask('Action')].mean(axis=0)\n",
        "romance_emb = anime_emb[genre_mask('Romance')].mean(axis=0)\n",
        "action_emb /= np.linalg.norm(action_emb)\n",
        "romance_emb /= np.linalg.norm(romance_emb)"
      ],
      "metadata": {
        "id": "BRRSbHGOTcwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = anime_emb @ action_emb\n",
        "y = anime_emb @ romance_emb\n",
        "action_mask = genre_mask('Action')\n",
        "romance_mask = genre_mask('Romance')\n",
        "\n",
        "c = np.full((anime_emb.shape[0], 3), [0.5, 0.5, 0.5])  # Gray color\n",
        "\n",
        "c[action_mask & ~romance_mask] = [1, 0, 0]  # Red\n",
        "c[~action_mask & romance_mask] = [0, 0, 1]  # Blue\n",
        "c[action_mask & romance_mask] = [0/255, 180/255, 0/255]  # Green\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "\n",
        "ax.scatter(x, y, c=c, s=1)\n",
        "\n",
        "ax.spines[\"left\"].set_position((\"data\", 0))\n",
        "ax.spines[\"bottom\"].set_position((\"data\", 0))\n",
        "\n",
        "ax.spines[\"top\"].set_visible(False)\n",
        "ax.spines[\"right\"].set_visible(False)\n",
        "\n",
        "ax.plot(1, 0, \">k\", transform=ax.get_yaxis_transform(), clip_on=False)\n",
        "ax.plot(0, 1, \"^k\", transform=ax.get_xaxis_transform(), clip_on=False)\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "ax.text(1.05, -1.4, 'Action', transform=ax.get_yaxis_transform(), ha='right', va='center')\n",
        "ax.text(0.1, 0.9, 'Romance', transform=ax.get_xaxis_transform(), ha='center', va='bottom')\n",
        "\n",
        "plt.savefig(f'{base_path}/school_hentai_plot.png', format='png', bbox_inches='tight', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NUeENTjF_bFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "reduced_data = sklearn.decomposition.PCA(n_components=2).fit_transform(anime_emb)"
      ],
      "metadata": {
        "id": "CBHaHtNEaRcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=genre_mask('Magic'), s=1)\n",
        "l = 5\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-5, 5)"
      ],
      "metadata": {
        "id": "pnYt5r6SaRe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rSxGxS5aRqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIjLxY0FaRtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k_values = range(1, 20)\n",
        "\n",
        "inertia = []\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(anime_emb)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, inertia, marker='o')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vxzJCUOd84_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFaG5jw0kCzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zbJaM5nkC27"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}